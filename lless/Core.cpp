#include "Core.h"
#include "Utils.h"
#include <float.h>
#include <map>

/****************************************************************************
* Core.cpp - part of the lless namespace, a general purpose
*            linear semi-markov structure prediction library
*
*   Copyright (C) 2002-2007  Axel E. Bernal (abernal@seas.upenn.edu)
*   
*   This program is free software; you can redistribute it and/or
*   modify it under the terms of the GNU General Public License as
*   published by the Free Software Foundation; either version 2 of the
*   License, or (at your option) any later version.
*   
*   This program is distributed in the hope that it will be useful, but
*   WITHOUT ANY WARRANTY; without even the implied warranty of
*   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
*   General Public License for more details.
*   
*   You should have received a copy of the GNU General Public License
*   along with this program; if not, write to the Free Software
*   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
*   02111-1307, USA.
*   
*   The GNU General Public License is contained in the file COPYING.
*   
*
****************************************************************************/


extern bool verbose;

namespace lless {

  /**
   * Core object constructor.
   */
  Core::Core( 
             FSM &fsm, 
             Lattice &lattice,
             ResourceEngine &re,
             FilterEngine &fe,
             FeatureEngine &fte,
             Evaluator &evaluator, 
             TagPrinter &printer,
             TAvgMethod avgMethod,
             TCombMethod combMethod,
             TStrand strand
             ) {

    _fsm = &fsm;
    _lattice = &lattice;
    _fe = &fe;
    _fte = &fte;
    _re = &re;
    _evaluator = &evaluator;
    _printer = &printer;
    _avgMethod = avgMethod;
    _combMethod = combMethod;
    _phi = 0.1;
    _r = 1;
    
    accum_gv = NULL;
    incorr_gv = NULL;
    pivot_gv = NULL;
    sigma_gv = NULL;
    accsigma_gv = NULL; 
    loStrand = 0, upStrand = NUM_STRANDS;

    if(strand != BOTH_STRANDS) {
      loStrand = strand;
      upStrand = loStrand + 1;
    }
  }


  /*
   * A member function to solve the quadratic program generated by MIRA
   * using the hildreth method.
   * @param gv the array of parameter vector updates, the ith entry in the
   * array corresponds to a update generated by the ith prediction.
   * @param loss the array of loss functions, each ith entry corresponds to
   * the loss incurred by ith prediction
   * @param K the size of loss and gv. It corresponds to the number k, 
   * when computing the k-best predictions.
   * @return the set of computed alphas [1..K] 
   */

  double *Core::hildreth(vector<GlobalVector *> &gv, vector<double> &loss, int K) {  
    double *alpha = new double[K];
    double *F = new double[K];
    double *kkt = new double[K];
    double max_kkt = DBL_MIN;
    int i;

    if(verbose)
      cerr << "In hildreth\n";

    double ** A = new double *[K];

    for(i = 0; i < K; i++) {
      A[i] = new double[K];
      for(int j = 0; j < K; j++) 
        A[i][j] = 0.0;
    }

    bool *is_computed = new bool [K];

    for(i = 0; i < K; i++) {
      A[i][i] = (*gv[i]) * (*gv[i]);
      is_computed[i] = false;
    }

    int max_kkt_i = -1;

    for(i = 0; i < K; i++) {
      alpha[i] = 0;
      F[i] = loss[i];
      kkt[i] = F[i];

      if(kkt[i] > max_kkt) {
        max_kkt = kkt[i];
        max_kkt_i = i;
      }

    }

    int iter = 0;
    double diff_alpha = 0;
    double try_alpha = 0;
    double add_alpha = 0;

    while(max_kkt >= EPS && iter < MAX_ITER) {

      if(verbose)
        cerr << "max_kkt_i = " << max_kkt_i << " max_kkt = " << max_kkt
             << " iter = " << iter << endl;

      diff_alpha = A[max_kkt_i][max_kkt_i] <= ZERO ?
        0.0 : 
        F[max_kkt_i]/A[max_kkt_i][max_kkt_i];

      try_alpha = alpha[max_kkt_i] + diff_alpha;
      add_alpha = 0.0;

      if(try_alpha < 0.0)
        add_alpha = -1.0 * alpha[max_kkt_i];
      else
        add_alpha = diff_alpha;

      alpha[max_kkt_i] = alpha[max_kkt_i] + add_alpha;

      if(!is_computed[max_kkt_i]) {
        for(i = 0; i < K; i++) {
          A[i][max_kkt_i] = (*gv[i]) * (*gv[max_kkt_i]);
          is_computed[i] = true;
        }
      }

      for(i = 0; i < K; i++) {
        F[i] -= add_alpha * A[i][max_kkt_i];
        kkt[i] = F[i];

        if(alpha[i] > ZERO)
          kkt[i] = fabs(F[i]);

      }

      max_kkt = DBL_MIN;
      max_kkt_i = -1;

      for(i = 0; i < K; i++)

        if(kkt[i] > max_kkt) {
          max_kkt = kkt[i];
          max_kkt_i = i;
        }

      iter++;
    }

    if(iter >= MAX_ITER)
      cerr << "Warning: QPSolver iterations >= MAX_ITER\n";

    delete [] F;
    delete [] kkt;
    delete [] is_computed;

    for(i = 0; i < K; i++)
      delete [] A[i];

    delete [] A;

    return alpha;
  }


  void Core::prePSequence(Sequence &c) {
    _fe->setSequence(c);
    /*
     * compute filters and features for current subannotSeq
     */
    for(int s = loStrand; s < upStrand; s++) {
      TStrand strand = (TStrand)s;
      _fe->computeSeqFilters(strand);
      _fte->preComputeFeatures(1, c.length(), strand);
    }
  }
  
  void Core::postPSequence(bool deleteSeqResources) {
    _fte->deletePreComputedFeatures();
    _fe->deleteSeqFilters();
    _fe->releaseSequence();

    if(deleteSeqResources)
      _re->deleteResourceSeqContents();

  }

  /**
   * Computes the feature vector gv out of sequence tagging seqTags. Filters
   * and features must have been computed before calling this function.
   * @param seqTags the input sequence taggin, i.e. a list of Tag objects.
   * @return the computed feature vector.
   */
  GlobalVector* Core::computeGlobalVector(SeqTags &seqTags,
					  double learnRate) {
    
    GlobalVector *gv = new GlobalVector(_fte->getFeatures(), true);
    _fte->setParamVector(gv);
    lattice()->updModelParams(seqTags, *_fte, learnRate);
    return gv;
  }


  /**
   * A member function the feature vector associated with Sequence annotSeq
   * and tagging listTags. Filter and features are computed within this
   * function. The vector to store the computed feature values must have
   * been initialized within object _fte and its values are not reset to
   * zero, so the caller function must take care of this.
   * @param annotSeq is the input sequence annotSeq.
   * @param listTags list of SeqTag objects, each of the elements of the list
   * represents an alternative labeling over annotSeq.
   * @param learnRate learnRate 
   * @return the vector which corresponds to the representation of
   * the first element of listTags (the canonical labeling of annotSeq). This
   * vector is computed through calls to 
   * Lattice::updModelParams(SeqTag &, FeatureEngine &, double)
   * \todo compute a vector of feature vectors, one for each entry in
   * listTags.
   */
  GlobalVector * Core::computeGlobalVector(Sequence &annotSeq, 
					   vector<SeqTags> &listTags,
					   double learnRate) {
    
    assert(_fte->getParamVector() != NULL);

    prePSequence(annotSeq);

    lattice()->updModelParams(listTags[0], *_fte, learnRate);

    postPSequence(true);
    
    return _fte->getParamVector();
  }

  /**
   * A member function that computes the minimum loss from a previously
   * computed array of losses
   */
  loss_t Core::computeMinLoss(vector<double> &losses, int numLosses) {

    loss_t p(-1, DBL_MAX);
    
    for(int i = 0; i < numLosses; i++) {
      /*
       * Choose the smallest loss among all the expected parses
       */
      if(p.second > losses[i])
        p = loss_t(i, losses[i]);
    }
    
    return p;
  }

  /**
   * A member function that computes the minimum loss from set any 
   * SeqTags objects in vtags to tags
   */
  loss_t Core::computeMinLoss(Sequence &c, 
			      SeqTags &tags, 
			      vector<SeqTags> &vtags,
			      int size) {

    if(size < 0)
      size = vtags.size();

    vector<double> losses = computeLosses(c, tags, vtags, size);
    return computeMinLoss(losses, size);
  }

  /**
   * A member function that computes the minimum loss from each element
   * of vtagsA to each element of vtagsB. Elements of vtagsB which 
   * are not in a minimum-loss relation to any element in vtagsA  
   * are left with a value of (-1, DBL_MAX)
   */
  vector<loss_t> Core::computeMinLoss(Sequence &c, 
				      vector<SeqTags> & vtagsA,
				      vector<SeqTags> & vtagsB,
				      int sizeB) {
    
    if(sizeB < 0) 
      sizeB = vtagsB.size();
    
    vector<loss_t> losses(sizeB,
			  loss_t(-1, DBL_MAX));
    
    for(int i = 0; i < vtagsA.size(); i++) { 
      loss_t min = Core::computeMinLoss(c, vtagsA[i],
					vtagsB, 
					sizeB);
      
      if(losses[min.first].first >= 0) {
	if(min.second < losses[min.first].second)
	  losses[min.first] = loss_t(i, min.second);

	continue;
      }

      losses[min.first] = loss_t(i, min.second);    
      
    }

    return losses;
  }

  
  /* Compute losses between SeqTags object tags and any element in vector
   * vtags
   * @param c the Sequence object where prediction occurred.
   * @param tags A tagging of c
   * @param vtags  a multiple tagging of c
   * @return the loss values from each elements of vtags to tags
   */
  vector<double> Core::computeLosses(Sequence &c, 
				     SeqTags &tags, 
				     vector<SeqTags> & vtags,
				     int size) {
        
    if(size < 0)
      size = vtags.size();

    vector<double> losses(size);
    for(int i = 0; i < size; i++)
      losses[i] = _evaluator->loss(c, vtags[i], tags, t);
    
    return losses;
  }
  
  /**
   * A member function that updates parameter vector using the PERPCEPTRON
   * method for training.
   * @param c the annotated input sequence
   * @param expTags the expected SeqTag object which tags c
   * @param predTags the predicted SeqTag object which tags c
   * @param learnRate the learning rate
   */
  void Core::updInstancePerceptron(Sequence &c, 
                                   vector<SeqTags> & expTags, 
                                   vector<SeqTags> & predTags, 
                                   double learnRate) {
    
    GlobalVector *params = _fte->currentParams();
    int numRestr = prepareParamUpdate(c, expTags, predTags, learnRate);

    assert(numRestr == gv2.size());

    for(int i = 0; i < numRestr; i++)
      (*params) += (*gv2[i]);

    _fte->setParamVector(params);
  }
  

  /**
   * A member function that updates parameter vector using the PEGASOS method
   * for training.
   * @param c the annotated input sequence
   * @param expTags the expected SeqTag object which tags c
   * @param predTags the predicted SeqTag object which tags c
   * @param t the absolute iteration number, since training started.
   * @param learnRate the learning rate. The regularization parameter lamba is
   * computed as the inverse of the learning rate.
   */
  void Core::updInstancePEGASOS(Sequence &c, 
                                vector<SeqTags> & expTags, 
                                vector<SeqTags> & predTags, 
                                double learnRate) {

    double lambda = 1/learnRate;
    GlobalVector *params = _fte->currentParams();
    int numRestr = prepareParamUpdate(c, expTags, predTags, 1);

    assert(numRestr == gv2.size());

    for(int i = 0; i < numRestr; i++) {
      double margin = (*params) * (*gv2[i]);
      if(margin < loss[i]) {
        (*params) *= (1.0 - 1.0/t);
        (*params) += (*gv2[i])*(1/(lambda*t));

        if(verbose)
          cerr << "1-alpha=" << (1-1.0/t) << " alpha=" << 1/(lambda*t) << " margin=" << margin << " loss=" << loss[i] << endl;
        
        double proj = 1/((sqrt(lambda))*sqrt((*params) * (*params)));

        if(verbose)
          cerr << "projection@" << i << " = min(1, " << proj << ")\n";

        if(proj < 1)
          (*params) *= proj;
      }
    }

    _fte->setParamVector(params);
  }


  /**
   * A member function that updates parameter vector using the MIRA method
   * for training. In case of  multilabeled sequences, the loss of each 
   * prediction (an element of predTags) is the  smallest loss among all the
   * predicted parses.
   * @param c the annotated input sequence
   * @param expTags the expected SeqTag object which tags c
   * @param predTags the predicted SeqTag object which tags c
   * @param learnRate the learning rate
   */
  void Core::updInstanceMIRA(Sequence &c, 
                             vector<SeqTags> & expTags, 
                             vector<SeqTags> & predTags, 
                             double learnRate) {

    GlobalVector *params = _fte->currentParams();

    int numRestr = prepareParamUpdate(c, expTags, predTags, learnRate);
    
    /* substracting { F(x, y) - F(x, y*) } dot w from the loss, so that we 
       can use hildreth without problems */        

    assert(numRestr == gv2.size());

    int i = 0;

    for( ; i < numRestr; i++) 
      loss[i] = loss[i] - (*params)*(*gv2[i]);

    double *alpha = hildreth(gv2, loss, numRestr);
    
    for(i = 0; i < numRestr; i++) {
      (*params) += ((*gv2[i])*alpha[i]);
      
      if(verbose)
        cerr << " alpha[" << i << "] = " << alpha[i] << endl;
    }

    delete [] alpha;

    _fte->setParamVector(params);
  }

  /**
   * A member function that updates parameter vector using the CWL method
   * for training. In case of  multilabeled sequences, the loss of each 
   * prediction (an element of predTags) is the  smallest loss among all the
   * predicted parses.
   * @param c the annotated input sequence
   * @param expTags the expected SeqTag object which tags c
   * @param predTags the predicted SeqTag object which tags c
   * @param learnRate the learning rate
   */
  void Core::updInstanceCWL(Sequence &c, 
                            vector<SeqTags> & expTags, 
                            vector<SeqTags> & predTags, 
                            double learnRate) {

    if(!sigma_gv) {
      sigma_gv = new GlobalVector(_fte->getFeatures(), true, 1.0);
      accsigma_gv = new GlobalVector(_fte->getFeatures());
    }

    GlobalVector *params = _fte->currentParams();
    GlobalVector sigma_x(_fte->getFeatures());

    int numRestr = prepareParamUpdate(c, expTags, predTags, learnRate);

    assert(numRestr == gv2.size());

    for(int i = 0; i < numRestr; i++) {
      sigma_x = *gv2[i];
      sigma_x.productInverse(*sigma_gv);
      double B = sigma_x * (*gv2[i]);
      double M = (*params) * (*gv2[i]);
      double gap = M - _phi*B;

      if(gap >= 0)
        continue;

      double c = 1 + 2*_phi*M;
      double alpha = (-c + sqrt(c*c - 8*_phi*gap))/(4*_phi*B);
      double factor = 2*alpha*_phi;

      if(verbose)
        cerr << "factor=" << factor << " alpha=" <<  alpha << " gap=" << gap << endl;

      // updating parameters
      (*params) += (sigma_x*alpha);  
      // updating sigma_gv
      (*sigma_gv) += (((*gv2[i]).product(*gv2[i]))*factor);

    }
      
    _fte->setParamVector(params);
  }

  /**
   * A member function that updates parameter vector using the ARROW method
   * for training. In case of  multilabeled sequences, the loss of each 
   * prediction (an element of predTags) is the  smallest loss among all the
   * predicted parses.
   * @param c the annotated input sequence
   * @param expTags the expected SeqTag object which tags c
   * @param predTags the predicted SeqTag object which tags c
   * @param learnRate the learning rate
   */
  void Core::updInstanceARROW(Sequence &c, 
                              vector<SeqTags> & expTags, 
                              vector<SeqTags> & predTags, 
                              double learnRate) {
    
    if(!sigma_gv) {
      sigma_gv = new GlobalVector(_fte->getFeatures(), true, 1.0);
      accsigma_gv = new GlobalVector(_fte->getFeatures());
    }

    GlobalVector *params = _fte->currentParams();
    GlobalVector sigma_x(_fte->getFeatures());
    GlobalVector sigma_x2(_fte->getFeatures());

    int numRestr = prepareParamUpdate(c, expTags, predTags, learnRate);

    assert(numRestr == gv2.size());

    for(int i = 0; i < numRestr; i++) {
      double M = (*params) * (*gv2[i]);
      double gap = loss[i] - M; // 1 - M;

      if(gap <= 0) 
	continue;

      double alpha = 0;
      
      sigma_x = *gv2[i];
      sigma_x.productInverse(*sigma_gv);
      sigma_x2 = sigma_x;
      sigma_x2.product(sigma_x2);
      double B = sigma_x * (*gv2[i]);
      double beta = 1.0/(B + _r);
      
      // updating parameters
      alpha = gap*beta;
      /*      GlobalVector sigma_x3(_fte->getFeatures());
      sigma_x3 = sigma_x;
      sigma_x3.highFilter(*sigma_gv, 1.05);*/
      (*params) += (sigma_x*alpha);
      
      if(verbose)
	cerr << " alpha=" <<  alpha << " beta=" << beta << " B=" << B << " gap=" << gap << " loss [" << i << "]=" << loss[i] << endl;      
      
      // updating sigma_gv
      sigma_gv->substractInverse(sigma_x2 * beta);
      
    }

    _fte->setParamVector(params);
  }


  void Core::initParamsFromLog(GlobalVector &finalParams,
                               char *logFile, int &iteration, 
                               int &accumIterations,
			       int addedFeatures) {
    
    std::ifstream paramStream(logFile);
    paramStream >> accumIterations >> iteration;

    /*
     * If more features have been added, we reset the iteration and 
     * usually (as specified in the feature file) only train parameters
     * tied to those new features, leaving all others 'frozen'.
     */
    if(addedFeatures)
      iteration = 0;

    pivot_gv = new GlobalVector(_fte->getFeatures(), &paramStream, addedFeatures);
    accum_gv = new GlobalVector(_fte->getFeatures(), &paramStream, addedFeatures);
    
    paramStream.get(); 
    
    if(!paramStream.eof()) {
      paramStream.unget();
      sigma_gv = new GlobalVector(_fte->getFeatures(), &paramStream, addedFeatures, true, 1.0);
      accsigma_gv = new GlobalVector(_fte->getFeatures(), &paramStream, addedFeatures);
    }

    paramStream.close();
    if(_combMethod == COMB_KL) {
      finalParams = *accum_gv;
      finalParams.productInverse(*accsigma_gv);
    }
    else
      finalParams.average(accumIterations, *accum_gv, true);
    
  }

  void Core::reStartTraining(GlobalVector  &finalParams,
                             list<Sequence *> &annotSeqs, 
                             TSetType trainSet,
                             TSetType predSet,
                             double learnRate, 
                             int maxIterations, 
                             int addedFeatures,
                             TTrainMethod tm, 
                             char *logFile,
                             char *paramsFile) {
    
    int iteration, accumIterations;
    initParamsFromLog(finalParams, logFile, iteration, accumIterations, addedFeatures);
    
    //    sigma_gv->print(cerr);
    //    _fte->freeze(0,-1);

    startTraining(finalParams, annotSeqs, trainSet,
                  predSet, learnRate, maxIterations,
		  tm, paramsFile, 
                  iteration, accumIterations);
    
  }
                             

  void Core::storeParams(char *paramsFile, int iteration,
                         GlobalVector *avg) {

    char tmp[500];
    strcpy(tmp, paramsFile);
    sprintf(tmp, "%s%d", tmp, iteration);
    ::ofstream paramStream(tmp);    
    _re->saveResources(paramStream);
    _fe->saveFilters(paramStream);
    _fte->saveFeatures(paramStream);
    _fte->setParamVector(avg);
    avg->store(paramStream);
    paramStream.close();    

  }

  void Core::storeLog(char *logFile, int iteration,
                      int accumIterations) {

    ::ofstream paramStream(logFile);    
    paramStream << accumIterations << " " << iteration << endl;
    pivot_gv->store(paramStream, true);
    accum_gv->store(paramStream, true);

    if(sigma_gv) {
      sigma_gv->store(paramStream);
      accsigma_gv->store(paramStream, true);
    }
    paramStream.close();
  }

  void Core::storeIteration(char *paramsFile, int iteration,
                            int accumIterations,
                            GlobalVector *avg,
                            bool saveLog) {

    storeParams(paramsFile, iteration, avg);

    if(saveLog) {
      char logFile[500];
      sprintf(logFile, "log_%s%d", paramsFile, iteration);
      storeLog(logFile, iteration, accumIterations);
    }
  }

  Core::~Core() {
    freetmpFeatVectors();
    if(accum_gv)
      delete accum_gv;
    if(sigma_gv)
      delete sigma_gv;
    if(accsigma_gv)
      delete accsigma_gv;
    if(pivot_gv)
      delete pivot_gv;
  }

}
